{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Scaling using .quantile() Pandas methods\n",
    "def scale_outlier(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    min_bound = Q1 - 1.5*IQR\n",
    "    max_bound = Q3 + 1.5*IQR\n",
    "    df[column] = np.where(df[column] > max_bound, max_bound, df[column])\n",
    "    df[column] = np.where(df[column] < min_bound, min_bound, df[column])\n",
    "\n",
    "# Min-Max Scaling using .min() and .max() Pandas methods\n",
    "def min_max_scaling(df):    \n",
    "    df_norm = df.copy()\n",
    "    for column in df_norm.columns:\n",
    "        df_norm[column] = (df_norm[column] - df_norm[column].min()) / (df_norm[column].max() - df_norm[column].min())        \n",
    "    return df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55990, 27)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Cleaned_data.csv\", index_col = [0]) # =>(55990, 34)\n",
    "target_col = ['BoEth', 'CoEth']\n",
    "remove_col = ['Bank', 'MSA', 'LocMedY', 'Assigned.ID']\n",
    "binary_encoding = ['SpcHsgGoals', 'Self', 'First']\n",
    "\n",
    "# ordinal_cols = ['CoBoCreditScor', 'BoCreditScor', 'NumBor']\n",
    "# categorical_cols = ['BoRace', 'CoRace', 'BoGender',\\\n",
    "#                   'CoGender', 'Geog', 'PropType', 'BoEth',\\\n",
    "#                   'CoEth', 'SpcHsgGoals', 'AcqTyp',' Bank',\\\n",
    "#                   'FedGuar', 'First', 'Self', 'MSA']\n",
    "# numerical_cols = ['Assigned.ID', 'BoAge', 'CoAge', 'MinPer', 'LocMedY', 'Tractrat', 'Income',\\\n",
    "#                  'IncRat', 'UPB', 'LTV', 'Term', 'Rate', 'Front','Back', 'PMI']\n",
    "\n",
    "# Removing columns not used for analysis\n",
    "df = df.drop(remove_col, axis = 1)\n",
    "\n",
    "# Extracting and removing the target column\n",
    "target = df[target_col]\n",
    "df = df.drop(target_col, axis = 1)\n",
    "\n",
    "# Binary encoding for specific columns\n",
    "df[binary_encoding] = df[binary_encoding].replace([2], 0)\n",
    "\n",
    "# print(df.shape) => (55990, 28)\n",
    "\n",
    "ordinal_cols = ['CoBoCreditScor', 'BoCreditScor', 'NumBor']\n",
    "# ordinal cols = 3\n",
    "\n",
    "categorical_cols = ['BoRace', 'CoRace', 'BoGender',\\\n",
    "                  'CoGender', 'Geog', 'PropType',\\\n",
    "                  'AcqTyp', 'FedGuar']\n",
    "# categorical cols  = 8\n",
    "\n",
    "numerical_cols = ['BoAge', 'CoAge', 'MinPer', 'Tractrat', 'Income',\\\n",
    "                 'IncRat', 'UPB', 'LTV', 'Term', 'Rate', 'Front','Back', 'PMI']\n",
    "#numerical cols = 13\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55990, 56)\n"
     ]
    }
   ],
   "source": [
    "# One hot encoding - Convert categorical columns to One hot encoding\n",
    "df = pd.get_dummies(df, columns = categorical_cols)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise numerical columns and add it to the dataset\n",
    "scale_columns = min_max_scaling(df[numerical_cols])\n",
    "df = df.drop(numerical_cols, axis = 1)\n",
    "df = pd.concat([df, scale_columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55990, 29)\n"
     ]
    }
   ],
   "source": [
    "# Drop one hot encoded columns that have values in less than 5% of total number of rows or more than 95% of rows\n",
    "oheEncodedCols = ['BoRace_1', 'BoRace_2', 'BoRace_3', 'BoRace_4', 'BoRace_5', 'BoRace_7',\\\n",
    "                  'CoRace_1', 'CoRace_2', 'CoRace_3', 'CoRace_4', 'CoRace_5', 'CoRace_7', 'CoRace_8',\\\n",
    "                  'BoGender_1', 'BoGender_2', 'BoGender_3', 'CoGender_1', 'CoGender_2', 'CoGender_3', 'CoGender_4',\\\n",
    "                  'Geog_1', 'Geog_2', 'PropType_PT01', 'PropType_PT02', 'PropType_PT04', 'PropType_PT06',\\\n",
    "                  'PropType_PT07', 'PropType_PT09', 'PropType_PT10', 'PropType_PT11', 'PropType_PT12',\\\n",
    "                  'AcqTyp_1', 'AcqTyp_4', 'FedGuar_0', 'FedGuar_1', 'FedGuar_2', 'FedGuar_3']\n",
    "for val in oheEncodedCols:\n",
    "  filteredLen = df[df[val] == 1].count()[0]\n",
    "  datasetLen = df.count()[0]\n",
    "  if( filteredLen < 0.3 * datasetLen or filteredLen > 0.97 * datasetLen):\n",
    "    df = df.drop([val], axis=1)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([df, target], axis = 1)\n",
    "new_df.to_csv(r'Encoded_data_without_PCA.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.738 0.085 0.031 0.021 0.02  0.018 0.014 0.013 0.01  0.008 0.008 0.007\n",
      " 0.005 0.004 0.004 0.003 0.003 0.003 0.001 0.001 0.001 0.001 0.001 0.\n",
      " 0.    0.    0.    0.    0.   ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "df_pca = pca.fit_transform(df)\n",
    "# Store as dataframe and print\n",
    "df_pca = pd.DataFrame(df_pca)\n",
    "df_pca.round(10).head()\n",
    "\n",
    "print(pca.explained_variance_ratio_.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "total_variance = 0\n",
    "pca_columns = 0\n",
    "for i in pca.explained_variance_ratio_:\n",
    "    total_variance += i\n",
    "    pca_columns += 1\n",
    "    if total_variance > 0.9:\n",
    "        break\n",
    "print(pca_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = df_pca[0:pca_columns + 1]\n",
    "df_pca.to_csv(r'Encoded_data_with_PCA.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
